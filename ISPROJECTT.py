# Load Dataset
def load_data():
    data = pd.read_csv('loan_approvals.csv')
    # Handle missing values
    data['Gender'] = data['Gender'].fillna('Unknown')
    data['Married'] = data['Married'].fillna('Unknown')
    data['Dependents'] = data['Dependents'].replace({'3+': 3})  # Convert '3+' to 3
    data['Dependents'] = data['Dependents'].fillna('0')
    data['Education'] = data['Education'].fillna('Unknown')
    data['Loan_Status'] = data['Loan_Status'].map({'Yes': 1, 'No': 0})

    # Apply Label Encoding for categorical features
    label_encoders = {}
    categorical_cols = ['Gender', 'Married', 'Dependents', 'Education']
    for col in categorical_cols:
        le = LabelEncoder()
        data[col] = le.fit_transform(data[col].astype(str))
        label_encoders[col] = le

    return data

data = load_data()

# Display sample data
st.write(data.head())

# Prepare Features and Target
X = data.drop('Loan_Status', axis=1)  # Features
y = data['Loan_Status']  # Target
X = X.fillna(X.mean())  # Handle missing numerical values

# Normalize the data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Model selection in Streamlit UI
model_choice = st.radio("Choose the model:", ('SVM', 'KNN', 'Random Forest'))

# Create and train the selected model
if model_choice == 'SVM':
    model = SVC()
elif model_choice == 'KNN':
    model = KNeighborsClassifier()
elif model_choice == 'Random Forest':
    model = RandomForestClassifier()

# Train the model
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Show accuracy result
accuracy = accuracy_score(y_test, y_pred)
st.write(f"Accuracy: {accuracy*100:.2f}%")

# UI Tabs for navigation
tabs = ['Machine learning', 'Neural network', 'Demo Machine learning', 'Demo Neural network']
selected_tab = st.selectbox('Choose a Tab', tabs)



# ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏ï‡∏≤‡∏°‡πÅ‡∏ó‡πá‡∏ö‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å
if selected_tab == 'Machine learning':
    if "svm_model" not in st.session_state:
        st.session_state.svm_model = SVC(kernel='rbf', C=1.0, gamma='scale')  # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• SVM ‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô
        st.session_state.svm_model.fit(X_train, y_train)  # ‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏• SVM ‡∏î‡πâ‡∏ß‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ù‡∏∂‡∏Å

    model = st.session_state.svm_model  # ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏ù‡∏∂‡∏Å‡πÅ‡∏•‡πâ‡∏ß‡∏à‡∏≤‡∏Å session_state
    st.header("Machine Learning")

    # ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏≤‡∏£‡∏≠‡∏ô‡∏∏‡∏°‡∏±‡∏ï‡∏¥‡∏™‡∏¥‡∏ô‡πÄ‡∏ä‡∏∑‡πà‡∏≠
    st.subheader("‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏≤‡∏£‡∏≠‡∏ô‡∏∏‡∏°‡∏±‡∏ï‡∏¥‡∏™‡∏¥‡∏ô‡πÄ‡∏ä‡∏∑‡πà‡∏≠ (Loan Approval Data)")
    st.write("""
    ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏≤‡∏£‡∏≠‡∏ô‡∏∏‡∏°‡∏±‡∏ï‡∏¥‡∏™‡∏¥‡∏ô‡πÄ‡∏ä‡∏∑‡πà‡∏≠ ‡∏ã‡∏∂‡πà‡∏á‡∏ö‡πà‡∏á‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤‡∏ú‡∏π‡πâ‡∏™‡∏°‡∏±‡∏Ñ‡∏£‡∏™‡∏¥‡∏ô‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏≠‡∏ô‡∏∏‡∏°‡∏±‡∏ï‡∏¥‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    ‡πÇ‡∏î‡∏¢‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏ï‡πà‡∏≤‡∏á‡πÜ ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏î‡∏±‡∏á‡∏Å‡∏•‡πà‡∏≤‡∏ß ‡πÇ‡∏î‡∏¢‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ï‡πà‡∏≤‡∏á‡πÜ ‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ:
    - **Gender**: ‡πÄ‡∏û‡∏®‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡∏™‡∏°‡∏±‡∏Ñ‡∏£ (‡∏ä‡∏≤‡∏¢/‡∏´‡∏ç‡∏¥‡∏á)
    - **Married**: ‡∏™‡∏ñ‡∏≤‡∏ô‡∏†‡∏≤‡∏û‡∏Å‡∏≤‡∏£‡∏™‡∏°‡∏£‡∏™ (‡πÅ‡∏ï‡πà‡∏á‡∏á‡∏≤‡∏ô/‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÅ‡∏ï‡πà‡∏á‡∏á‡∏≤‡∏ô)
    - **Dependents**: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•‡∏ó‡∏µ‡πà‡∏ú‡∏π‡πâ‡∏™‡∏°‡∏±‡∏Ñ‡∏£‡∏û‡∏∂‡πà‡∏á‡∏û‡∏≤ (‡πÄ‡∏ä‡πà‡∏ô ‡πÄ‡∏î‡πá‡∏Å ‡∏´‡∏£‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏î‡∏π‡πÅ‡∏•)
    - **Education**: ‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤ (‡∏õ‡∏£‡∏¥‡∏ç‡∏ç‡∏≤‡∏ï‡∏£‡∏µ/‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤)
    - **Income**: ‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡∏™‡∏°‡∏±‡∏Ñ‡∏£
    - **Loan_Amount**: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏á‡∏¥‡∏ô‡∏ó‡∏µ‡πà‡∏ú‡∏π‡πâ‡∏™‡∏°‡∏±‡∏Ñ‡∏£‡∏Ç‡∏≠‡∏™‡∏¥‡∏ô‡πÄ‡∏ä‡∏∑‡πà‡∏≠
    - **Loan_Status**: ‡∏Å‡∏≤‡∏£‡∏≠‡∏ô‡∏∏‡∏°‡∏±‡∏ï‡∏¥‡∏™‡∏¥‡∏ô‡πÄ‡∏ä‡∏∑‡πà‡∏≠ (‡∏ú‡πà‡∏≤‡∏ô/‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô)
    """)

    # ‡∏Å‡∏≤‡∏£‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    st.write("‡∏Ñ‡∏∏‡∏ì‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏≤‡∏£‡∏≠‡∏ô‡∏∏‡∏°‡∏±‡∏ï‡∏¥‡∏™‡∏¥‡∏ô‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏î‡πâ‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà:")

# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏•‡∏¥‡∏Å‡πÑ‡∏õ‡∏¢‡∏±‡∏á Google Drive
    drive_link = 'https://drive.google.com/drive/u/0/folders/1M49d6uX87fUK6YtjgN-TmAFZ-hjZ33wb'

# ‡πÉ‡∏ä‡πâ st.markdown ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏™‡∏î‡∏á‡∏•‡∏¥‡∏á‡∏Å‡πå
    st.markdown(f"[‡∏Ñ‡∏•‡∏¥‡∏Å‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡∏¥‡∏î Google Drive]( {drive_link} )")

    # ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÇ‡∏Ñ‡πâ‡∏î‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    st.code("""
import pandas as pd
data = pd.read_csv('loan_approvals.csv')
data.head()  # ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ö‡∏≤‡∏á‡∏™‡πà‡∏ß‡∏ô
    """, language='python')

    # ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Missing Values
    st.write("‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏´‡∏≤‡∏¢‡πÑ‡∏õ (Missing Values) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå")
    st.code("""
# ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏´‡∏≤‡∏¢‡πÑ‡∏õ‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç
data.fillna(data.mean(numeric_only=True), inplace=True)

# ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏´‡∏≤‡∏¢‡πÑ‡∏õ‡πÉ‡∏ô‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà (‡πÄ‡∏ä‡πà‡∏ô Gender, Married, Dependents) ‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ 'Unknown'
data.fillna({'Gender': 'Unknown', 'Married': 'Unknown', 'Dependents': '0'}, inplace=True)
    """, language='python')

    # ‡∏Å‡∏≤‡∏£‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà (Categorical Data) ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç
    st.write("‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÉ‡∏ä‡πâ Label Encoding ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç ‡∏ã‡∏∂‡πà‡∏á‡∏à‡∏∞‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏î‡πâ")
    st.code("""
from sklearn.preprocessing import LabelEncoder
categorical_cols = ['Gender', 'Married', 'Dependents', 'Education']

# ‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç
for col in categorical_cols:
    le = LabelEncoder()
    data[col] = le.fit_transform(data[col])
    """, language='python')

    # ‡πÅ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô Features ‡πÅ‡∏•‡∏∞ Target
    st.write("‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÅ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏≠‡∏Å‡πÄ‡∏õ‡πá‡∏ô Features (‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢) ‡πÅ‡∏•‡∏∞ Target (‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢)")
    st.code("""
# Features ‡∏Ñ‡∏∑‡∏≠‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏¢‡∏Å‡πÄ‡∏ß‡πâ‡∏ô‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå (Loan_Status)
X = data.drop(columns=['Loan_Status'])

# Target ‡∏Ñ‡∏∑‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå (Loan_Status) ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢
y = LabelEncoder().fit_transform(data['Loan_Status'])  # ‡πÅ‡∏õ‡∏•‡∏á Loan_Status ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç
    """, language='python')

    # ‡∏Å‡∏≤‡∏£‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡πÅ‡∏•‡∏∞‡∏ó‡∏î‡∏™‡∏≠‡∏ö
    st.write("‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÅ‡∏¢‡∏Å Features ‡πÅ‡∏•‡∏∞ Target ‡πÅ‡∏•‡πâ‡∏ß ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏≠‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å (Training Data) ‡πÅ‡∏•‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏î‡∏™‡∏≠‡∏ö (Test Data)")
    st.code("""
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    """, language='python')

    # ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Feature Scaling)
    st.write("‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏• ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏õ‡∏£‡∏±‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡πâ‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á -1 ‡πÅ‡∏•‡∏∞ 1 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô")
    st.code("""
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)  # ‡∏õ‡∏£‡∏±‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ä‡∏∏‡∏î‡∏ù‡∏∂‡∏Å
X_test = scaler.transform(X_test)  # ‡∏õ‡∏£‡∏±‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ä‡∏∏‡∏î‡∏ó‡∏î‡∏™‡∏≠‡∏ö
    """, language='python')

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• SVM
    st.write("‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• SVM (Support Vector Machine) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ß‡πà‡∏≤ ‡∏ú‡∏π‡πâ‡∏™‡∏°‡∏±‡∏Ñ‡∏£‡∏à‡∏∞‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏≠‡∏ô‡∏∏‡∏°‡∏±‡∏ï‡∏¥‡∏™‡∏¥‡∏ô‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà")
    st.code("""
from sklearn.svm import SVC
svm = SVC(class_weight='balanced')  # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö SVM
svm.fit(X_train, y_train)  # ‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏î‡πâ‡∏ß‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ä‡∏∏‡∏î‡∏ù‡∏∂‡∏Å
y_pred = svm.predict(X_test)  # ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏î‡πâ‡∏ß‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ä‡∏∏‡∏î‡∏ó‡∏î‡∏™‡∏≠‡∏ö
print("Accuracy:", accuracy_score(y_test, y_pred))  # ‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥
    """, language='python')

    # ‡πÉ‡∏ä‡πâ GridSearchCV ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏Ç‡∏≠‡∏á SVM
    st.write("‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ GridSearchCV ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏≠‡∏á‡∏Ñ‡πà‡∏≤‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ï‡πà‡∏≤‡∏á‡πÜ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö SVM ‡πÅ‡∏•‡∏∞ KNN")
    st.code("""
from sklearn.model_selection import GridSearchCV
param_grid_svm = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf'], 'gamma': ['scale', 'auto']}
grid_search_svm = GridSearchCV(SVC(), param_grid_svm, cv=5)  # ‡πÉ‡∏ä‡πâ Cross Validation ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤‡∏Ñ‡πà‡∏≤‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
grid_search_svm.fit(X_train, y_train)  # ‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•
print("Best Parameters for SVM:", grid_search_svm.best_params_)  # ‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡πà‡∏≤‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
    """, language='python')

    # ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ï‡πà‡∏≤‡∏á‡πÜ
    st.write("‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢ ‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ï‡πà‡∏≤‡∏á‡πÜ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡∏ß‡πà‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏î‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î")
    st.code("""
models = {'SVM': grid_search_svm.best_estimator_}
for name, model in models.items():
    model.fit(X_train, y_train)  # ‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•
    y_pred = model.predict(X_test)  # ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
    print(f"Model: {name}")
    print("Accuracy:", accuracy_score(y_test, y_pred))  # ‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥
    print("Classification Report:\n", classification_report(y_test, y_pred))  # ‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥
    """, language='python')



elif selected_tab == 'Neural network':
    st.header("üß† Neural Network Model - MNIST")

    st.subheader("üìå 1Ô∏è‚É£ ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• MNIST")
    st.write("‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• **MNIST** ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç 0-9 ‡∏Ç‡∏ô‡∏≤‡∏î 28x28 ‡∏û‡∏¥‡∏Å‡πÄ‡∏ã‡∏•")
    st.code("""
    from keras.datasets import mnist
    (x_train, y_train), (x_test, y_test) = mnist.load_data()
    """, language="python")

    st.subheader("üìå 2Ô∏è‚É£ ‡∏ó‡∏≥ Normalization ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•")
    st.write("‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡πà‡∏≤‡∏û‡∏¥‡∏Å‡πÄ‡∏ã‡∏•‡∏à‡∏≤‡∏Å **0-255** ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô **0-1** ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô")
    st.code("""
    x_train, x_test = x_train / 255.0, x_test / 255.0
    """, language="python")

    st.subheader("üìå 3Ô∏è‚É£ ‡∏õ‡∏£‡∏±‡∏ö‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•")
    st.write("‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô `(28, 28, 1)` ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö Convolutional Layers ‡πÅ‡∏•‡∏∞‡∏ó‡∏≥ One-hot Encoding ‡∏Ç‡∏≠‡∏á Labels")
    st.code("""
    x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)
    x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)

    from keras.utils import to_categorical
    y_train = to_categorical(y_train, 10)
    y_test = to_categorical(y_test, 10)
    """, language="python")

    st.subheader("üìå 4Ô∏è‚É£ ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• CNN")
    st.write("‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• **Convolutional Neural Network (CNN)** ‡πÇ‡∏î‡∏¢‡∏°‡∏µ 2 ‡∏ä‡∏±‡πâ‡∏ô‡∏Ç‡∏≠‡∏á Convolutional Layers ‡πÅ‡∏•‡∏∞ MaxPooling")
    st.code("""
    from keras.models import Sequential
    from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense

    model = Sequential([
        Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
        MaxPooling2D(2, 2),
        Conv2D(64, (3, 3), activation='relu'),
        MaxPooling2D(2, 2),
        Flatten(),
        Dense(128, activation='relu'),
        Dense(10, activation='softmax')
    ])
    """, language="python")

    st.subheader("üìå 5Ô∏è‚É£ ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Optimizer ‡πÅ‡∏•‡∏∞ Compile ‡πÇ‡∏°‡πÄ‡∏î‡∏•")
    st.write("‡πÉ‡∏ä‡πâ `SGD` (Stochastic Gradient Descent) ‡∏û‡∏£‡πâ‡∏≠‡∏° `Momentum` ‡πÅ‡∏•‡∏∞‡∏Å‡∏≥‡∏´‡∏ô‡∏î Loss Function ‡πÄ‡∏õ‡πá‡∏ô `categorical_crossentropy`")
    st.code("""
    from keras.optimizers import SGD

    model.compile(optimizer=SGD(learning_rate=0.01, momentum=0.9),
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])
    """, language="python")

    st.subheader("üìå 6Ô∏è‚É£ ‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•")
    st.write("‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• `x_train` ‡πÅ‡∏•‡∏∞ `y_train` ‡∏ó‡∏≥‡∏Å‡∏≤‡∏£ Train ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ß‡∏•‡∏≤ 2 Epochs")
    st.code("""
    model.fit(x_train, y_train, epochs=2)
    """, language="python")

    st.subheader("üìå 7Ô∏è‚É£ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏à‡∏≤‡∏Å‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î")
    st.write("‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏†‡∏≤‡∏û, ‡πÅ‡∏õ‡∏•‡∏á‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô 28x28, ‡∏ó‡∏≥ Normalization ‡πÅ‡∏•‡∏∞‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç")
    st.code("""
    from google.colab import files
    from PIL import Image
    import numpy as np
    import matplotlib.pyplot as plt

    uploaded = files.upload()

    for fn in uploaded.keys():
        img = Image.open(fn).convert('L')  # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô grayscale
        img = img.resize((28, 28))
        img_array = np.array(img) / 255.0
        img_array = img_array.reshape(1, 28, 28, 1)

        prediction = model.predict(img_array)
        predicted_label = np.argmax(prediction)

        print(f"Prediction: {predicted_label}")
        plt.imshow(img, cmap='gray')
        plt.title(f"Predicted Number: {predicted_label}")
        plt.show()
    """, language="python")

    st.write("‚úÖ **‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ô‡∏µ‡πâ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏à‡∏≤‡∏Å‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏î‡πâ! üéØ**")

elif selected_tab == 'Demo Machine learning':
    st.header("Machine Learning Demo")
    st.subheader("Support Vector Machine")
    # ‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ
    gender = st.selectbox('Gender', ['Male', 'Female'])
    marital_status = st.selectbox('Marital Status', ['Yes', 'No'])
    education = st.selectbox('Education Level', ['Graduate', 'Not Graduate'])
    dependents = st.selectbox('Dependents', ['0', '1', '2', '3+'])
    income = st.number_input("Income", min_value=0)
    loan_amount = st.number_input("Loan Amount", min_value=0)

    # ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ
    def get_user_input(gender, marital_status, education, dependents, income, loan_amount):
        try:
            gender = label_encoders['Gender'].transform([gender])[0]
        except ValueError:
            gender = label_encoders['Gender'].transform(['Unknown'])[0]
        
        try:
            marital_status = label_encoders['Married'].transform([marital_status])[0]
        except ValueError:
            marital_status = label_encoders['Married'].transform(['Unknown'])[0]
        
        dependents = '3' if dependents == '3+' else dependents
        try:
            dependents = label_encoders['Dependents'].transform([str(dependents)])[0]
        except ValueError:
            dependents = label_encoders['Dependents'].transform(['0'])[0]
        
        try:
            education = label_encoders['Education'].transform([education])[0]
        except ValueError:
            education = label_encoders['Education'].transform(['Unknown'])[0]
        
        # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢
        user_data = np.array([[gender, marital_status, dependents, education, income, loan_amount]])
        user_data = scaler.transform(user_data)  # ‡∏õ‡∏£‡∏±‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        prediction = svm.predict(user_data)
        
        return prediction

    # ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏•‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏•‡∏¥‡∏Å‡∏õ‡∏∏‡πà‡∏°
    if st.button('Predict'):
        prediction = get_user_input(gender, marital_status, education, dependents, income, loan_amount)
        if prediction == 1:
            st.success("Loan Approved")
        else:
            st.error("Loan Denied")


import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense
from keras.optimizers import SGD
from PIL import Image
import numpy as np
import streamlit as st
import matplotlib.pyplot as plt  # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏Å‡∏£‡∏≤‡∏ü

# ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏•‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å session
def load_model_and_data():
    if 'model' not in st.session_state:
        # ‡πÇ‡∏´‡∏•‡∏î‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• MNIST ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏• Neural Network
        (x_train, y_train), (x_test, y_test) = mnist.load_data()

        # ‡∏ó‡∏≥ Normalization
        x_train, x_test = x_train / 255.0, x_test / 255.0

        # ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô (28, 28, 1) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Conv2D
        x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)
        x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)

        # One-hot encode labels
        y_train = to_categorical(y_train, 10)
        y_test = to_categorical(y_test, 10)

        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•
        model = Sequential([
            Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
            MaxPooling2D(2, 2),
            Conv2D(64, (3, 3), activation='relu'),
            MaxPooling2D(2, 2),
            Flatten(),
            Dense(128, activation='relu'),
            Dense(10, activation='softmax')
        ])

        # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ optimizer ‡πÅ‡∏•‡∏∞ compile ‡πÇ‡∏°‡πÄ‡∏î‡∏•
        model.compile(optimizer=SGD(learning_rate=0.01, momentum=0.9), 
                      loss='categorical_crossentropy', 
                      metrics=['accuracy'])

        # ‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏• ‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• history
        history = model.fit(x_train, y_train, epochs=2)

        # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•
        test_loss, test_acc = model.evaluate(x_test, y_test)
        print(f"Test accuracy: {test_acc}")

        # ‡πÄ‡∏Å‡πá‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏ô session
        st.session_state.model = model
        st.session_state.history = history
        st.session_state.x_test = x_test
        st.session_state.y_test = y_test
    else:
        model = st.session_state.model
        history = st.session_state.history
        x_test = st.session_state.x_test
        y_test = st.session_state.y_test

    return model, history, x_test, y_test

# ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ù‡∏∂‡∏Å‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß ‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢
if selected_tab == 'Demo Neural network':
    model, history, x_test, y_test = load_model_and_data()

    st.header("Neural Network Demo")

    st.write(""" 
        ‡πÉ‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏à‡∏≤‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏• Neural Network ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏ù‡∏∂‡∏Å‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• MNIST
        ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ô‡∏µ‡πâ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏à‡∏≤‡∏Å‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤‡πÑ‡∏î‡πâ
    """)

    # ‡πÅ‡∏™‡∏î‡∏á‡∏Å‡∏£‡∏≤‡∏ü Accuracy ‡πÅ‡∏•‡∏∞ Loss ‡∏Å‡πà‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î
    st.header("Training History")

    # ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏Å‡∏£‡∏≤‡∏ü‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå
    col1, col2 = st.columns(2)

    with col1:
        # ‡∏Å‡∏£‡∏≤‡∏ü Accuracy
        fig, ax = plt.subplots(figsize=(6, 4))
        ax.plot(history.history['accuracy'], label='Training Accuracy')
        ax.set_title('Model Accuracy')
        ax.set_xlabel('Epochs')
        ax.set_ylabel('Accuracy')
        ax.legend()
        st.pyplot(fig)

    with col2:
        # ‡∏Å‡∏£‡∏≤‡∏ü Loss
        fig, ax = plt.subplots(figsize=(6, 4))
        ax.plot(history.history['loss'], label='Training Loss')
        ax.set_title('Model Loss')
        ax.set_xlabel('Epochs')
        ax.set_ylabel('Loss')
        ax.legend()
        st.pyplot(fig)

    # ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏ß‡∏≤‡∏á‡πÉ‡∏ô‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå
    col1, col2 = st.columns([2, 3])

    with col1:
        st.image("https://upload.wikimedia.org/wikipedia/commons/a/a5/MnistExample.png", caption="‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å MNIST", use_container_width=True)

    with col2:
        st.write("""
            ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ô‡∏µ‡πâ‡∏ñ‡∏π‡∏Å‡∏ù‡∏∂‡∏Å‡∏î‡πâ‡∏ß‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å MNIST ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏≠‡∏á‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡∏°‡∏∑‡∏≠
            ‡πÇ‡∏î‡∏¢‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ô‡∏µ‡πâ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏à‡∏≤‡∏Å‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏î‡πâ
        """)

    # ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏à‡∏≤‡∏Å‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ
    uploaded = st.file_uploader("‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢", type=["jpg", "png", "jpeg"])

    if uploaded is not None:
        # ‡πÅ‡∏õ‡∏•‡∏á‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î
        img = Image.open(uploaded).convert('L')  # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô grayscale
        img = img.resize((28, 28))  # ‡∏õ‡∏£‡∏±‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î‡πÄ‡∏õ‡πá‡∏ô 28x28
        img_array = np.array(img)  # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô numpy array
        img_array = img_array / 255.0  # Normalization
        img_array = img_array.reshape(1, 28, 28, 1)  # ‡∏õ‡∏£‡∏±‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô (1, 28, 28, 1)

        # ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏•‡∏à‡∏≤‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•
        prediction = model.predict(img_array)
        predicted_label = np.argmax(prediction)

        # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢  
        st.image(img, caption='Uploaded Image', use_container_width=True)  # ‡πÅ‡∏Å‡πâ‡πÄ‡∏õ‡πá‡∏ô use_container_width=True
        st.markdown(f"<h1 style='text-align: center; font-size: 60px;'>Predicted Number: {predicted_label}</h1>", unsafe_allow_html=True)





